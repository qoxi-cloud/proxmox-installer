# shellcheck shell=bash
# =============================================================================
# QEMU installation and boot functions
# =============================================================================

# Checks if system is booted in UEFI mode.
# Returns: 0 if UEFI, 1 if legacy BIOS
is_uefi_mode() {
  [[ -d /sys/firmware/efi ]]
}

# Configures QEMU settings (shared between install and boot).
# Detects UEFI/BIOS mode, KVM availability, CPU cores, and RAM.
# Side effects: Sets UEFI_OPTS, KVM_OPTS, CPU_OPTS, QEMU_CORES, QEMU_RAM, DRIVE_ARGS
setup_qemu_config() {
  log "Setting up QEMU configuration"

  # UEFI configuration
  if is_uefi_mode; then
    UEFI_OPTS="-bios /usr/share/ovmf/OVMF.fd"
    log "UEFI mode detected"
  else
    UEFI_OPTS=""
    log "Legacy BIOS mode"
  fi

  # KVM acceleration
  KVM_OPTS="-enable-kvm"
  CPU_OPTS="-cpu host"
  log "Using KVM acceleration"

  # CPU and RAM configuration
  local available_cores available_ram_mb
  available_cores=$(nproc)
  available_ram_mb=$(free -m | awk '/^Mem:/{print $2}')
  log "Available cores: $available_cores, Available RAM: ${available_ram_mb}MB"

  # Use override values if provided, otherwise auto-detect
  if [[ -n $QEMU_CORES_OVERRIDE ]]; then
    QEMU_CORES="$QEMU_CORES_OVERRIDE"
    log "Using user-specified cores: $QEMU_CORES"
  else
    # Use all available cores for QEMU
    QEMU_CORES=$available_cores
    [[ $QEMU_CORES -lt $MIN_CPU_CORES ]] && QEMU_CORES=$MIN_CPU_CORES
  fi

  if [[ -n $QEMU_RAM_OVERRIDE ]]; then
    QEMU_RAM="$QEMU_RAM_OVERRIDE"
    log "Using user-specified RAM: ${QEMU_RAM}MB"
    # Warn if requested RAM exceeds available
    if [[ $QEMU_RAM -gt $((available_ram_mb - QEMU_MIN_RAM_RESERVE)) ]]; then
      print_warning "Requested QEMU RAM (${QEMU_RAM}MB) may exceed safe limits (available: ${available_ram_mb}MB)"
    fi
  else
    # Use all available RAM minus reserve for host
    QEMU_RAM=$((available_ram_mb - QEMU_MIN_RAM_RESERVE))
    [[ $QEMU_RAM -lt $MIN_QEMU_RAM ]] && QEMU_RAM=$MIN_QEMU_RAM
  fi

  log "QEMU config: $QEMU_CORES vCPUs, ${QEMU_RAM}MB RAM"

  # Create/load virtio mapping (shared with make_answer_toml)
  load_virtio_mapping

  # Build DRIVE_ARGS from virtio mapping
  DRIVE_ARGS=""

  # Add boot disk first (if separate)
  if [[ -n $BOOT_DISK ]]; then
    DRIVE_ARGS="$DRIVE_ARGS -drive file=$BOOT_DISK,format=raw,media=disk,if=virtio"
  fi

  # Add pool disks
  for drive in "${ZFS_POOL_DISKS[@]}"; do
    DRIVE_ARGS="$DRIVE_ARGS -drive file=$drive,format=raw,media=disk,if=virtio"
  done

  log "Drive args: $DRIVE_ARGS"
}

# =============================================================================
# Drive release helper functions
# =============================================================================

# Internal: sends signal to process if running.
# Parameters:
#   $1 - Process ID
#   $2 - Signal name/number
#   $3 - Log message
_signal_process() {
  local pid="$1"
  local signal="$2"
  local message="$3"

  if kill -0 "$pid" 2>/dev/null; then
    log "$message"
    kill "-$signal" "$pid" 2>/dev/null || true
  fi
}

# Internal: kills processes by pattern with graceful then forced termination.
# Parameters:
#   $1 - Process pattern to match
_kill_processes_by_pattern() {
  local pattern="$1"
  local pids

  pids=$(pgrep -f "$pattern" 2>/dev/null || true)
  if [[ -n $pids ]]; then
    log "Found processes matching '$pattern': $pids"

    # Graceful shutdown first (SIGTERM)
    for pid in $pids; do
      _signal_process "$pid" "TERM" "Sending TERM to process $pid"
    done
    sleep 3

    # Force kill if still running (SIGKILL)
    for pid in $pids; do
      _signal_process "$pid" "9" "Force killing process $pid"
    done
    sleep 1
  fi

  # Also try pkill as fallback
  pkill -TERM "$pattern" 2>/dev/null || true
  sleep 1
  pkill -9 "$pattern" 2>/dev/null || true
}

# Internal: stops mdadm RAID arrays.
_stop_mdadm_arrays() {
  if ! command -v mdadm &>/dev/null; then
    return 0
  fi

  log "Stopping mdadm arrays..."
  mdadm --stop --scan 2>/dev/null || true

  # Stop specific arrays if found
  for md in /dev/md*; do
    if [[ -b $md ]]; then
      mdadm --stop "$md" 2>/dev/null || true
    fi
  done
}

# Internal: deactivates LVM volume groups.
_deactivate_lvm() {
  if ! command -v vgchange &>/dev/null; then
    return 0
  fi

  log "Deactivating LVM volume groups..."
  vgchange -an 2>/dev/null || true

  # Deactivate specific VGs by name if vgs is available
  if command -v vgs &>/dev/null; then
    while IFS= read -r vg; do
      if [[ -n $vg ]]; then vgchange -an "$vg" 2>/dev/null || true; fi
    done < <(vgs --noheadings -o vg_name 2>/dev/null)
  fi
}

# Internal: unmounts filesystems on target drives.
_unmount_drive_filesystems() {
  [[ -z ${DRIVES[*]} ]] && return 0

  log "Unmounting filesystems on target drives..."
  for drive in "${DRIVES[@]}"; do
    # Use findmnt for efficient mount point detection (faster and more reliable)
    if command -v findmnt &>/dev/null; then
      while IFS= read -r mountpoint; do
        [[ -z $mountpoint ]] && continue
        log "Unmounting $mountpoint"
        umount -f "$mountpoint" 2>/dev/null || true
      done < <(findmnt -rn -o TARGET "$drive"* 2>/dev/null)
    else
      # Fallback to mount | grep
      local drive_name
      drive_name=$(basename "$drive")
      while IFS= read -r mountpoint; do
        [[ -z $mountpoint ]] && continue
        log "Unmounting $mountpoint"
        umount -f "$mountpoint" 2>/dev/null || true
      done < <(mount | grep -E "(^|/)$drive_name" | awk '{print $3}')
    fi
  done
}

# Internal: kills processes holding drives open.
_kill_drive_holders() {
  [[ -z ${DRIVES[*]} ]] && return 0

  log "Checking for processes using drives..."
  for drive in "${DRIVES[@]}"; do
    # Use lsof if available
    if command -v lsof &>/dev/null; then
      while IFS= read -r pid; do
        [[ -z $pid ]] && continue
        _signal_process "$pid" "9" "Killing process $pid using $drive"
      done < <(lsof "$drive" 2>/dev/null | awk 'NR>1 {print $2}' | sort -u)
    fi

    # Use fuser as alternative
    if command -v fuser &>/dev/null; then
      fuser -k "$drive" 2>/dev/null || true
    fi
  done
}

# =============================================================================
# Main drive release function
# =============================================================================

# Releases drives from existing locks before QEMU starts.
# Stops RAID arrays, deactivates LVM, unmounts filesystems, kills holders.
release_drives() {
  log "Releasing drives from locks..."

  # Kill QEMU processes
  _kill_processes_by_pattern "qemu-system-x86"

  # Stop RAID arrays
  _stop_mdadm_arrays

  # Deactivate LVM
  _deactivate_lvm

  # Unmount filesystems
  _unmount_drive_filesystems

  # Additional pause for locks to release
  sleep 2

  # Kill any remaining processes holding drives
  _kill_drive_holders

  log "Drives released"
}

# Installs Proxmox via QEMU with autoinstall ISO.
# Runs QEMU in background with direct drive access.
# Side effects: Writes to drives, exits on failure
install_proxmox() {
  # Run preparation in background to show progress immediately
  local qemu_config_file
  qemu_config_file=$(mktemp)

  (
    # Setup QEMU configuration
    setup_qemu_config

    # Save config for parent shell (including all QEMU variables)
    cat >"$qemu_config_file" <<EOF
QEMU_CORES=$QEMU_CORES
QEMU_RAM=$QEMU_RAM
UEFI_MODE=$(is_uefi_mode && echo "yes" || echo "no")
KVM_OPTS='$KVM_OPTS'
UEFI_OPTS='$UEFI_OPTS'
CPU_OPTS='$CPU_OPTS'
DRIVE_ARGS='$DRIVE_ARGS'
EOF

    # Verify ISO exists
    if [[ ! -f "./pve-autoinstall.iso" ]]; then
      print_error "Autoinstall ISO not found!"
      exit 1
    fi

    # Release any locks on drives before QEMU starts
    release_drives
  ) &
  local prep_pid=$!

  # Wait for config file to be ready
  local timeout=10
  while [[ ! -s $qemu_config_file ]] && ((timeout > 0)); do
    sleep 0.1
    ((timeout--))
  done

  # Load QEMU configuration
  if [[ -s $qemu_config_file ]]; then
    # shellcheck disable=SC1090
    source "$qemu_config_file"
    rm -f "$qemu_config_file"
  fi

  show_progress $prep_pid "Starting QEMU (${QEMU_CORES} vCPUs, ${QEMU_RAM}MB RAM)" "QEMU started (${QEMU_CORES} vCPUs, ${QEMU_RAM}MB RAM)"

  # Add subtasks after preparation completes
  if [[ $UEFI_MODE == "yes" ]]; then
    live_log_subtask "UEFI mode detected"
  else
    live_log_subtask "Legacy BIOS mode"
  fi
  live_log_subtask "KVM acceleration enabled"
  live_log_subtask "Configured ${QEMU_CORES} vCPUs, ${QEMU_RAM}MB RAM"

  # Now start QEMU in parent process (not in subshell) - this is KEY!
  # shellcheck disable=SC2086
  qemu-system-x86_64 $KVM_OPTS $UEFI_OPTS \
    $CPU_OPTS -smp "$QEMU_CORES" -m "$QEMU_RAM" \
    -boot d -cdrom ./pve-autoinstall.iso \
    $DRIVE_ARGS -no-reboot -display none >qemu_install.log 2>&1 &

  local qemu_pid=$!

  # Give QEMU a moment to start or fail
  sleep 2

  # Check if QEMU is still running
  if ! kill -0 $qemu_pid 2>/dev/null; then
    log "ERROR: QEMU failed to start"
    log "QEMU install log:"
    cat qemu_install.log >>"$LOG_FILE" 2>&1
    exit 1
  fi

  show_progress "$qemu_pid" "Installing Proxmox VE" "Proxmox VE installed"
  local exit_code=$?

  # Verify installation completed (QEMU exited cleanly)
  if [[ $exit_code -ne 0 ]]; then
    log "ERROR: QEMU installation failed with exit code $exit_code"
    log "QEMU install log:"
    cat qemu_install.log >>"$LOG_FILE" 2>&1
    exit 1
  fi
}

# Boots installed Proxmox with SSH port forwarding.
# Exposes SSH on port 5555 for post-install configuration.
# Side effects: Starts QEMU, sets QEMU_PID global
boot_proxmox_with_port_forwarding() {
  setup_qemu_config

  # Check if port is already in use
  if ! check_port_available "$SSH_PORT"; then
    print_error "Port $SSH_PORT is already in use"
    log "ERROR: Port $SSH_PORT is already in use"
    exit 1
  fi

  # shellcheck disable=SC2086
  nohup qemu-system-x86_64 $KVM_OPTS $UEFI_OPTS \
    $CPU_OPTS -device e1000,netdev=net0 \
    -netdev user,id=net0,hostfwd=tcp::5555-:22 \
    -smp "$QEMU_CORES" -m "$QEMU_RAM" \
    $DRIVE_ARGS -display none \
    >qemu_output.log 2>&1 &

  QEMU_PID=$!

  # Wait for port to be open first (in background for show_progress)
  (
    local timeout=300
    local elapsed=0
    while ((elapsed < timeout)); do
      # Suppress all connection errors by redirecting to /dev/null
      if exec 3<>/dev/tcp/localhost/5555 2>/dev/null; then
        exec 3<&- # Close the file descriptor
        exit 0
      fi 2>/dev/null
      sleep 3
      ((elapsed += 3))
    done
    exit 1
  ) 2>/dev/null &
  local wait_pid=$!

  show_progress $wait_pid "Booting installed Proxmox" "Proxmox booted"
  local exit_code=$?

  if [[ $exit_code -ne 0 ]]; then
    log "ERROR: Timeout waiting for SSH port"
    log "QEMU output log:"
    cat qemu_output.log >>"$LOG_FILE" 2>&1
    return 1
  fi

  # Wait for SSH to be fully ready (handles key exchange timing)
  wait_for_ssh_ready 120 || {
    log "ERROR: SSH connection failed"
    log "QEMU output log:"
    cat qemu_output.log >>"$LOG_FILE" 2>&1
    return 1
  }
}
